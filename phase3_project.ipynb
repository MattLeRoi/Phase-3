{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to identify folks before churn so we can reach out. Err on side of identifying too many. Aim for most potential churners with limit of 3:1 ratio of false positives to true positives.\n",
    "\n",
    "The total net value of a customer is ~$200. Estimated lost revenue due to churn is ~$80/customer. Cost of outreach is ~$20/customer. A 3:1 ratio of false positives to true positives will be approximately net neutral, with lower ratios leading to more profits. The maximum value lies in maximizing the churn prevented and minimizing the cost of outreach to misidentified churners. The formula for profit/loss on this project would be: 80*TP - 20*(TP+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('bigml_59c28831336c6604c800002a.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Churn rate: ',round(df['churn'].value_counts()[1]/3333,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for categorical columns\n",
    "categorical = ['state','area code', 'international plan', 'voice mail plan','churn']\n",
    "df_encoded = pd.get_dummies(df, columns=categorical, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop(['churn_True','phone number'], axis=1)\n",
    "y = df_encoded.churn_True\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=1, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model_name,X_test,y_test):\n",
    "    y_hat_test = model_name.predict(X_test)\n",
    "    conf_mat = confusion_matrix(y_test, y_hat_test)\n",
    "#     ConfusionMatrixDisplay(conf_mat).plot();\n",
    "    rec_score = recall_score(y_test,y_hat_test)\n",
    "    profit_loss = round(80*conf_mat[1,1]-20*(conf_mat[0,1]+conf_mat[1,1]),2)\n",
    "#     print(f'Profit(loss): ${profit_loss}')\n",
    "#     print(f'Recall: {round(rec_score,3)}')\n",
    "    \n",
    "    return(profit_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_base = LogisticRegression(random_state=42,fit_intercept=False, C=1e10, solver='liblinear')\n",
    "logreg_base.fit(X_train.values,y_train) # added .values to handle labeling error\n",
    "\n",
    "model_eval(logreg_base,X_test.values,y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling everything to a 0-1 range\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=42,fit_intercept=False, C=1e10, solver='liblinear')\n",
    "logreg.fit(X_train_scaled,y_train)\n",
    "model_eval(logreg,X_test_scaled,y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now let's compare a few different ratios of minority class to majority class\n",
    "ratios = [0.25, 0.33, 0.5, 0.7, 1]\n",
    "names = ['0.25', '0.33','0.5','0.7','even']\n",
    "colors = sns.color_palette('Set2')\n",
    "\n",
    "for n, ratio in enumerate(ratios):\n",
    "    # Fit a model\n",
    "    smote = SMOTE(sampling_strategy=ratio, random_state=7)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train) \n",
    "    X_test_resampled, y_test_resampled = smote.fit_resample(X_test_scaled, y_test)\n",
    "    logreg = LogisticRegression(fit_intercept=False, C=1e10, solver ='liblinear')\n",
    "    model_log = logreg.fit(X_train_resampled, y_train_resampled)\n",
    "    print(f'Ratio: {names[n]}')\n",
    "    model_eval(logreg,X_test_scaled,y_test);\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ratios = np.arange(.4,.6,.01)\n",
    "\n",
    "for n, ratio in enumerate(ratios):\n",
    "    # Fit a model\n",
    "    smote = SMOTE(sampling_strategy=ratio, random_state=7)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train) \n",
    "    X_test_resampled, y_test_resampled = smote.fit_resample(X_test_scaled, y_test)\n",
    "    logreg = LogisticRegression(fit_intercept=False, C=1e10, solver ='liblinear')\n",
    "    model_log = logreg.fit(X_train_resampled, y_train_resampled)\n",
    "    print(f'Ratio: {round(ratios[n],3)}')\n",
    "    model_eval(logreg,X_test_scaled,y_test);\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy=.52, random_state=7)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train) \n",
    "logreg_resample = LogisticRegression(fit_intercept=False, C=1e10, solver ='liblinear')\n",
    "logreg_resample.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "model_eval(logreg_resample,X_test_scaled,y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1, figsize = (6,6), dpi=900)\n",
    "tree.plot_tree(clf,\n",
    "               class_names=np.unique(y).astype('str'),\n",
    "               filled = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval(clf,X_test,y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = clf.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_hat_test)\n",
    "ConfusionMatrixDisplay(conf_mat).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_g = DecisionTreeClassifier(criterion='gini',random_state=42)\n",
    "clf_g.fit(X_train, y_train)\n",
    "model_eval(clf_g,X_test,y_test);\n",
    "\n",
    "clf_ll = DecisionTreeClassifier(criterion='log_loss',random_state=42)\n",
    "clf_ll.fit(X_train, y_train)\n",
    "model_eval(clf_ll,X_test,y_test);\n",
    "# No help, stick with 'entropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = np.arange(.2,1.1,.1)\n",
    "\n",
    "for n, ratio in enumerate(ratios):\n",
    "    # Fit a model\n",
    "    smote = SMOTE(sampling_strategy=round(ratio,3), random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train) \n",
    "    X_test_resampled, y_test_resampled = smote.fit_resample(X_test_scaled, y_test)\n",
    "    clf_resamp_temp = DecisionTreeClassifier(criterion='entropy',random_state=7)\n",
    "    clf_resamp_temp.fit(X_train_resampled, y_train_resampled)\n",
    "    print(f'Ratio: {round(ratios[n],3)}')\n",
    "    model_eval(clf_resamp_temp,X_test_scaled,y_test);\n",
    "    print('\\n')\n",
    "\n",
    "    # Slight improvement in recall, but not better than baseline in terms of profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=37\n",
    "\n",
    "def hypertuning(parameter,X_train,y_train,X_test,y_test,start,stop,increment):\n",
    "    parameter_range = np.arange(start,stop,increment)\n",
    "    temp_range=[]\n",
    "    test_aucs=[]\n",
    "    train_aucs=[]\n",
    "    profit=[]\n",
    "    entropy='entropy'\n",
    "    for i in parameter_range:\n",
    "        param_name = parameter\n",
    "        param_value = i\n",
    "        \n",
    "        # With assistance from ChatGPT - it gave me this method to set parameter names using variables so I can \n",
    "        # call this function with any parameter I want and have it run through a range of parameters.\n",
    "        kwargs = {param_name: param_value, 'random_state':SEED,'criterion':entropy}\n",
    "        dt_temp = DecisionTreeClassifier(**kwargs)\n",
    "        # Thanks ChatGPT!!\n",
    "        \n",
    "        dt_temp.fit(X_train,y_train)\n",
    "        y_pred_train = dt_temp.predict(X_train)\n",
    "        y_pred_test = dt_temp.predict(X_test)\n",
    "        auc_train_temp = round(roc_auc_score(y_train, y_pred_train),3)\n",
    "        auc_test_temp = round(roc_auc_score(y_test, y_pred_test),3)\n",
    "        temp_range.append(i)\n",
    "        test_aucs.append(auc_test_temp)\n",
    "        train_aucs.append(auc_train_temp)\n",
    "        profit.append(model_eval(dt_temp,X_test,y_test))\n",
    "        \n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(temp_range,test_aucs,label='Test')\n",
    "    ax1.plot(temp_range,train_aucs,label='Train')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(temp_range,profit,label='Profit',color='g')\n",
    "    ax1.set_ylabel('AUC')\n",
    "    ax2.set_ylabel('Profit ($)')\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypertuning(\"max_depth\",X_train,y_train,X_test,y_test,1,33,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypertuning(\"min_samples_split\",X_train,y_train,X_test,y_test,0.0001,.25,.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypertuning(\"min_samples_leaf\",X_train,y_train,X_test,y_test,1,25,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypertuning(\"max_features\",X_train,y_train,X_test,y_test,1,75,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_final = DecisionTreeClassifier(random_state=SEED,criterion='entropy',max_depth=7)\n",
    "dt_final.fit(X_train,y_train)\n",
    "model_eval(dt_final,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = dt_final.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_hat_test)\n",
    "ConfusionMatrixDisplay(conf_mat).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Negative log loss doesn't exist as something we can import,\n",
    "# but we can create it\n",
    "neg_log_loss = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "# Instantiate the model (same as previous example)\n",
    "baseline_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Create a list to hold the score from each fold\n",
    "kfold_scores = np.ndarray(5)\n",
    "\n",
    "# Instantiate a splitter object and loop over its result\n",
    "kfold = StratifiedKFold()\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    # Extract train and validation subsets using the provided indices\n",
    "    X_t, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_t, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Clone the provided model and fit it on the train subset\n",
    "    temp_model = clone(baseline_model)\n",
    "    temp_model.fit(X_t, y_t)\n",
    "    \n",
    "    # Evaluate the provided model on the validation subset\n",
    "    neg_log_loss_score = neg_log_loss(temp_model, X_val, y_val)\n",
    "    kfold_scores[fold] = neg_log_loss_score\n",
    "    \n",
    "-(kfold_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameter_range = np.arange(1,25,1)\n",
    "temp_range=[]\n",
    "test_aucs=[]\n",
    "train_aucs=[]\n",
    "profit=[]\n",
    "entropy='entropy'\n",
    "for i in parameter_range:\n",
    "#     to_run = \"dt_temp = DecisionTreeClassifier(random_state=SEED,criterion=entropy,\"\n",
    "#     to_run += parameter\n",
    "#     to_run += \"=\"\n",
    "#     to_run += str(i)\n",
    "#     to_run+= \")\"\n",
    "#     print(to_run)\n",
    "#     exec(to_run)\n",
    "    dt_temp = DecisionTreeClassifier(random_state=SEED,criterion=entropy, min_samples_leaf=i)\n",
    "    dt_temp.fit(X_train,y_train)\n",
    "    y_pred_train = dt_temp.predict(X_train)\n",
    "    y_pred_test = dt_temp.predict(X_test)\n",
    "    auc_train_temp = round(roc_auc_score(y_train, y_pred_train),3)\n",
    "    auc_test_temp = round(roc_auc_score(y_test, y_pred_test),3)\n",
    "    temp_range.append(i)\n",
    "    test_aucs.append(auc_test_temp)\n",
    "    train_aucs.append(auc_train_temp)\n",
    "    profit.append(model_eval(dt_temp,X_test,y_test));\n",
    "    \n",
    "    conf_mat = confusion_matrix(y_test, y_pred_test)\n",
    "#     ConfusionMatrixDisplay(conf_mat).plot();\n",
    "    \n",
    "plt.plot(temp_range,profit);\n",
    "# plt.plot(temp_range,test_aucs,label='Test')\n",
    "# plt.plot(temp_range,train_aucs,label='Train')\n",
    "# plt.legend();"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
